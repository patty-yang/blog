 ## HTTP 1.0

HTTP 1.0 存在两个主要的技术限制：

🔒 **无法复用连接**

每个 HTTP 请求都需要建立一个全新的 TCP 连接，这带来了显著的性能开销：

1. 连接的建立和销毁都会占用服务器和客户端的资源，会造成内存资源的浪费
2. 连接的建立和销毁都会消耗时间，造成响应时间的浪费

🚫 **队头阻塞**

- 客户端必须等待当前请求完成后，才能发送下一个请求
- 这种机制导致请求队列容易形成阻塞，降低了资源加载效率

## HTTP 1.1

为了解决 HTTP1.0 的问题，HTTP1.1 引入了长连接机制，允许在同一个 TCP 连接上进行多次请求和响应交互

🔌 **连接管理**

TCP 连接可通过以下三种方式关闭：

1. 显式设置 `Connection: close` 头部字段
2. 客户端定期发送心跳包维持连接，心跳停止则服务器关闭连接
3. 服务器检测到客户端长时间无活动，主动关闭连接

📦 **管道化传输**

HTTP 1.1 支持管道化（Pipelining）技术，允许客户端在收到上一个响应之前就发送下一个请求，这种机制：

- ✅ 显著减少了网络延迟
- ❌ 仍然存在队头阻塞问题

为了缓解队头阻塞，常用的优化策略：

1. 减少文件数量，减少对头阻塞的几率
2. 开辟多个 TCP 连接，实现真正的并行传输

> 💡 由于多个请求发送的是同一个 TCP 连接，服务器必须按照请求到达的先后顺序进行响应,这是造成队头阻塞的根本原因。

## HTTP 2.0

🔄 **二进制分帧与多路复用**

HTTP/2.0 引入了全新的二进制分帧层，将通信数据拆分为更小的单元进行传输：

- 帧(Frame): 数据传输的最小单位
- 流(Stream): 一个完整的请求/响应数据流，由多个帧组成
- 每个流都有唯一标识符，帧会携带所属流的信息
- 多个流可以并行传输，实现真正的多路复用，彻底解决队头阻塞问题

📦 **头部压缩**

为了提升网络传输效率，HTTP/2.0 采用 HPACK 算法对头部进行压缩：

- 建立动态字典，相同的头部字段只需传输索引
- 静态字典预定义常用头部字段
- 显著减少了请求中冗余的头部数据

🚀 **服务器推送**

HTTP/2.0 支持服务器主动推送资源(Server Push)：

- 服务器可以预测客户端需要的资源
- 无需等待客户端请求即可主动推送
- 有效减少了客户端的请求延迟

<!--
HTTP 各版本的区别
HTTP1.0
每次请求都要建立连接，浪费资源
如果前次请求被阻塞了，会导致后续请求无法发出，造成对头阻塞问题

HTTP1.1

1. 引入了长连接，默认开启长连接，减少了 TCP 连接的建立和销毁的开销
1. 基于长连接的基础，引入了管道化，允许在响应到达之前发送下一个请求，但是要保证响应的顺序
1. 引入了缓存机制，Cache-Control
1. 断点传输

HTTP2.0

1. 二进制分帧
1. 多路复用
1. 头部压缩
1. 服务器推送 -->
